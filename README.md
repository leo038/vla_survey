# VLA（Vision-Language-Action）模型发展全景汇总表

主表仅保留常用列，点击每行 **「详情」** 可展开论文全称、规模、输入/输出与核心贡献，无需横向拖动。

---

## 一、萌芽阶段（2022–2023）

<table style="border-collapse: collapse; width: 100%;">
<thead><tr><th>序号</th><th>工作名称</th><th>提出时间</th><th>研究机构</th><th>是否开源</th><th>链接</th><th>详情</th></tr></thead>
<tbody>
<tr><td>1</td><td>CLIPort</td><td>2021.09 (CoRL 2021)</td><td>University of Washington, NVIDIA</td><td>是</td><td><a href="https://arxiv.org/abs/2109.12098">arXiv:2109.12098</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> CLIPort: What and Where Pathways for Robotic Manipulation<br><strong>规模</strong> — &nbsp; <strong>输入</strong> RGB图像, 自然语言指令 &nbsp; <strong>输出</strong> 机器人操作动作（pick-and-place）<br><strong>核心贡献</strong> 结合CLIP语义理解与Transporter Networks的空间精度，实现语言条件下的操作</div></details></td></tr>
<tr><td>2</td><td>BC-Z</td><td>2022.02 (CoRL 2021)</td><td>Google Research</td><td>是</td><td><a href="https://arxiv.org/abs/2202.02005">arXiv:2202.02005</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning<br><strong>规模</strong> — &nbsp; <strong>输入</strong> RGB图像, 自然语言指令/视频演示 &nbsp; <strong>输出</strong> 机器人操作动作<br><strong>核心贡献</strong> 通过大规模数据收集(25,000+机器人demo)实现零样本任务泛化</div></details></td></tr>
<tr><td>3</td><td>GATO</td><td>2022.05 (TMLR 2022)</td><td>DeepMind</td><td>否</td><td><a href="https://arxiv.org/abs/2205.06175">arXiv:2205.06175</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> A Generalist Agent<br><strong>规模</strong> 1.2B &nbsp; <strong>输入</strong> 多模态（图像, 文本, 本体感知, 关节力矩等） &nbsp; <strong>输出</strong> 多模态（文本, 动作, 关节力矩等）<br><strong>核心贡献</strong> 单一Transformer网络完成604种不同任务（Atari、图像描述、对话、机器人控制等）</div></details></td></tr>
<tr><td>4</td><td>VIMA</td><td>2022.10 (ICML 2023)</td><td>Stanford, NVIDIA, Caltech, 清华, UT Austin</td><td>是</td><td><a href="https://arxiv.org/abs/2210.03094">arXiv:2210.03094</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> VIMA: General Robot Manipulation with Multimodal Prompts<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 多模态提示（交错的文本和视觉token） &nbsp; <strong>输出</strong> 机器人动作（自回归）<br><strong>核心贡献</strong> 通过多模态提示统一多种机器人操作任务，零样本泛化提升2.9倍</div></details></td></tr>
<tr><td>5</td><td>RT-1</td><td>2022.12</td><td>Google Research, Everyday Robots</td><td>是</td><td><a href="https://arxiv.org/abs/2212.06817">arXiv:2212.06817</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> RT-1: Robotics Transformer for Real-World Control at Scale<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 相机图像, 自然语言任务指令 &nbsp; <strong>输出</strong> 电机控制命令<br><strong>核心贡献</strong> 基于Transformer的大规模机器人控制，130,000个episode、13个机器人、17个月数据训练</div></details></td></tr>
<tr><td>6</td><td>UniPi</td><td>2023.01 (NeurIPS 2023 Spotlight)</td><td>Google Brain</td><td>—</td><td><a href="https://arxiv.org/abs/2302.00111">arXiv:2302.00111</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Learning Universal Policies via Text-Guided Video Generation<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 文本描述, 当前图像帧 &nbsp; <strong>输出</strong> 视频序列（通过逆动力学提取动作）<br><strong>核心贡献</strong> 将序列决策问题转化为文本条件视频生成问题，视频作为跨环境通用接口</div></details></td></tr>
<tr><td>7</td><td>ACT</td><td>2023.04 (RSS 2023)</td><td>Stanford, Meta, UC Berkeley</td><td>是</td><td><a href="https://arxiv.org/abs/2304.13705">arXiv:2304.13705</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware<br><strong>规模</strong> ~80M &nbsp; <strong>输入</strong> RGB图像（多相机）, 关节位置, 本体感知 &nbsp; <strong>输出</strong> 动作块（未来k步动作序列）<br><strong>核心贡献</strong> 预测固定长度的动作序列而非单步动作，提高时序连贯性和样本效率</div></details></td></tr>
<tr><td>8</td><td>Diffusion Policy</td><td>2023.03 (RSS 2023)</td><td>Columbia University, TRI, MIT</td><td>是</td><td><a href="https://arxiv.org/abs/2303.04137">arXiv:2303.04137</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Diffusion Policy: Visuomotor Policy Learning via Action Diffusion<br><strong>规模</strong> — &nbsp; <strong>输入</strong> RGB图像, 机器人观测 &nbsp; <strong>输出</strong> 机器人动作（去噪扩散过程）<br><strong>核心贡献</strong> 将视觉运动策略表示为条件去噪扩散过程，平均性能提升46.9%</div></details></td></tr>
<tr><td>9</td><td>RT-2</td><td>2023.07</td><td>Google DeepMind</td><td>否</td><td><a href="https://arxiv.org/abs/2307.15818">arXiv:2307.15818</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 相机图像, 自然语言指令 &nbsp; <strong>输出</strong> 机器人动作（文本token形式）<br><strong>核心贡献</strong> 在VLM(PaLI-X/PaLM-E)上联合微调机器人数据和网络数据，将动作表示为语言token</div></details></td></tr>
<tr><td>10</td><td>RT-X</td><td>2023.10 (ICRA 2024)</td><td>Open X-Embodiment Collaboration (21家机构)</td><td>是</td><td><a href="https://arxiv.org/abs/2310.08864">arXiv:2310.08864</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Open X-Embodiment: Robotic Learning Datasets and RT-X Models<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 多机器人观测（图像、本体感知等） &nbsp; <strong>输出</strong> 跨具身机器人动作<br><strong>核心贡献</strong> 21家机构、22种机器人、527种技能的标准化数据集，证明跨机器人正迁移</div></details></td></tr>
<tr><td>11</td><td>RoboFlamingo</td><td>2023.08 (ICLR 2024)</td><td>—</td><td>是</td><td><a href="https://arxiv.org/abs/2308.01390">arXiv:2308.01390</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Vision-Language Foundation Models as Effective Robot Imitators<br><strong>规模</strong> 3B/4B/9B（基于OpenFlamingo） &nbsp; <strong>输入</strong> 图像, 自然语言指令 &nbsp; <strong>输出</strong> 机器人操作动作<br><strong>核心贡献</strong> 将预训练VLM(OpenFlamingo)适配用于机器人模仿学习，在CALVIN上达SOTA</div></details></td></tr>
<tr><td>12</td><td>LEO</td><td>2023.11 (ICML 2024)</td><td>BIGAI, 北京大学, CMU, 清华</td><td>是</td><td><a href="https://arxiv.org/abs/2311.12871">arXiv:2311.12871</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> An Embodied Generalist Agent in 3D World<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 2D图像（自我中心）, 3D点云, 语言指令 &nbsp; <strong>输出</strong> 机器人动作, 3D描述, 导航命令<br><strong>核心贡献</strong> 3D世界中的多模态通才Agent，融合2D/3D视觉编码器和LLM</div></details></td></tr>
<tr><td>13</td><td>GR-1</td><td>2023.12 (ICLR 2024)</td><td>ByteDance</td><td>是</td><td><a href="https://arxiv.org/abs/2312.13139">arXiv:2312.13139</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 语言指令, 观测图像序列, 机器人状态 &nbsp; <strong>输出</strong> 机器人动作, 未来图像<br><strong>核心贡献</strong> GPT风格Transformer进行大规模视频生成预训练+机器人数据微调，CALVIN 94.9%成功率</div></details></td></tr>
</tbody>
</table>

---

## 二、探索阶段（2024年）

<table style="border-collapse: collapse; width: 100%;">
<thead><tr><th>序号</th><th>工作名称</th><th>提出时间</th><th>研究机构</th><th>是否开源</th><th>链接</th><th>详情</th></tr></thead>
<tbody>
<tr><td>14</td><td>3D-VLA</td><td>2024.03</td><td>UMass Amherst</td><td>是</td><td><a href="https://arxiv.org/abs/2403.09631">arXiv:2403.09631</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> 3D-VLA: A 3D Vision-Language-Action Generative World Model<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 3D点云, 图像, 语言指令 &nbsp; <strong>输出</strong> 机器人动作, 目标图像, 目标点云<br><strong>核心贡献</strong> 引入3D世界模型的VLA，在规划动作前推理未来场景</div></details></td></tr>
<tr><td>15</td><td>LLARVA</td><td>2024.06 (CoRL 2024)</td><td>UC Berkeley</td><td>是</td><td><a href="https://arxiv.org/abs/2406.11815">arXiv:2406.11815</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 图像, 语言指令, 本体感知 &nbsp; <strong>输出</strong> 机器人动作, 视觉轨迹<br><strong>核心贡献</strong> 引入视觉-动作指令微调和"视觉轨迹"中间表征</div></details></td></tr>
<tr><td>16</td><td>Octo</td><td>2024.05</td><td>多机构 (Octo Team)</td><td>是</td><td><a href="https://arxiv.org/abs/2405.12213">arXiv:2405.12213</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Octo: An Open-Source Generalist Robot Policy<br><strong>规模</strong> 27M/93M &nbsp; <strong>输入</strong> 图像（腕部/第三方相机）, 语言/目标图像, 本体感知 &nbsp; <strong>输出</strong> 机器人动作<br><strong>核心贡献</strong> 开源Transformer扩散策略，800k轨迹训练，可在消费级GPU上高效微调</div></details></td></tr>
<tr><td>17</td><td>RT-H</td><td>2024.03 (RSS 2024)</td><td>Google DeepMind, Stanford</td><td>—</td><td><a href="https://arxiv.org/abs/2403.01823">arXiv:2403.01823</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> RT-H: Action Hierarchies Using Language<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 视觉观测, 语言指令 &nbsp; <strong>输出</strong> 语言运动描述（中间层）, 机器人动作<br><strong>核心贡献</strong> 用语言运动作为高级任务和低级动作间的中间层，支持人类语言纠正</div></details></td></tr>
<tr><td>18</td><td>OpenVLA</td><td>2024.06</td><td>Stanford, UC Berkeley, MIT等</td><td>是</td><td><a href="https://arxiv.org/abs/2406.09246">arXiv:2406.09246</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> OpenVLA: An Open-Source Vision-Language-Action Model<br><strong>规模</strong> 7B &nbsp; <strong>输入</strong> 图像（SigLIP+DINOv2）, 语言指令 &nbsp; <strong>输出</strong> token化机器人动作<br><strong>核心贡献</strong> 开源7B VLA，以7倍少的参数超越RT-2-X(55B) 16.5%成功率</div></details></td></tr>
<tr><td>19</td><td>RoboMamba</td><td>2024.06 (NeurIPS 2024)</td><td>—</td><td>是</td><td><a href="https://arxiv.org/abs/2406.04339">arXiv:2406.04339</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 图像, 语言指令 &nbsp; <strong>输出</strong> 机器人动作, SE(3)位姿<br><strong>核心贡献</strong> 用Mamba(SSM)架构实现3倍推理加速，仅需0.1%参数微调</div></details></td></tr>
<tr><td>20</td><td>ECoT</td><td>2024.07 (CoRL 2024)</td><td>Stanford, UC Berkeley</td><td>是</td><td><a href="https://arxiv.org/abs/2407.08693">arXiv:2407.08693</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Robotic Control via Embodied Chain-of-Thought Reasoning<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 图像, 语言指令, 机器人状态 &nbsp; <strong>输出</strong> 多步推理, 机器人动作<br><strong>核心贡献</strong> 训练VLA在执行动作前进行具身推理，OpenVLA成功率提升28%</div></details></td></tr>
<tr><td>21</td><td>Gen2Act</td><td>2024.09 (CoRL 2025)</td><td>Google DeepMind, CMU, Stanford</td><td>—</td><td><a href="https://arxiv.org/abs/2409.16283">arXiv:2409.16283</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 语言指令, 图像 &nbsp; <strong>输出</strong> 生成的人类演示视频, 机器人动作<br><strong>核心贡献</strong> 两阶段：先生成人类演示视频，再基于视频执行策略，实现零样本泛化</div></details></td></tr>
<tr><td>22</td><td>TinyVLA</td><td>2024.09 (RA-L 2025)</td><td>—</td><td>是</td><td><a href="https://arxiv.org/abs/2409.12514">arXiv:2409.12514</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 图像, 语言指令 &nbsp; <strong>输出</strong> 机器人动作（扩散策略解码器）<br><strong>核心贡献</strong> 紧凑型VLA，推理速度比OpenVLA快20倍，无需大规模预训练</div></details></td></tr>
<tr><td>23</td><td>HPT</td><td>2024.09 (NeurIPS 2024 Spotlight)</td><td>MIT CSAIL, Meta AI</td><td>是</td><td><a href="https://arxiv.org/abs/2409.20537">arXiv:2409.20537</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers<br><strong>规模</strong> 1B &nbsp; <strong>输入</strong> 本体感知, 视觉（不同具身） &nbsp; <strong>输出</strong> 机器人控制动作<br><strong>核心贡献</strong> 预训练共享Transformer主干学习任务和具身无关表征，52个数据集200k+轨迹</div></details></td></tr>
<tr><td>24</td><td>RDT</td><td>2024.10</td><td>清华大学</td><td>是</td><td><a href="https://arxiv.org/abs/2410.07864">arXiv:2410.07864</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation<br><strong>规模</strong> 1.2B &nbsp; <strong>输入</strong> RGB图像(3视角), 语言指令, 控制频率, 本体感知 &nbsp; <strong>输出</strong> 机器人动作（预测64步）<br><strong>核心贡献</strong> 最大的扩散基础模型用于双臂操作，物理可解释统一动作空间</div></details></td></tr>
<tr><td>25</td><td>LAPA</td><td>2024.10</td><td>KAIST, UW, Microsoft, NVIDIA, AI2</td><td>是</td><td><a href="https://arxiv.org/abs/2410.11758">arXiv:2410.11758</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Latent Action Pretraining from Videos<br><strong>规模</strong> 7B &nbsp; <strong>输入</strong> RGB图像, 语言指令 &nbsp; <strong>输出</strong> 潜在动作→机器人动作<br><strong>核心贡献</strong> 首个无需动作标签的VLA预训练方法，从视频学习潜在动作，效率提升30倍</div></details></td></tr>
<tr><td>26</td><td>HiRT</td><td>2024.09</td><td>清华大学, UC Berkeley</td><td>—</td><td><a href="https://arxiv.org/abs/2410.05273">arXiv:2410.05273</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers<br><strong>规模</strong> InstructBLIP 7B + 轻量策略 &nbsp; <strong>输入</strong> RGB图像, 语言指令 &nbsp; <strong>输出</strong> 机器人动作<br><strong>核心贡献</strong> 层级架构：低频VLM语义理解+高频轻量策略实时控制，推理延迟降低58%</div></details></td></tr>
<tr><td>27</td><td>π₀</td><td>2024.10</td><td>Physical Intelligence</td><td>是</td><td><a href="https://arxiv.org/abs/2410.24164">arXiv:2410.24164</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> π₀: A Vision-Language-Action Flow Model for General Robot Control<br><strong>规模</strong> — &nbsp; <strong>输入</strong> RGB图像, 语言指令 &nbsp; <strong>输出</strong> 连续机器人动作（flow matching）<br><strong>核心贡献</strong> 基于flow matching的VLA，继承VLM语义知识，跨平台通用机器人控制</div></details></td></tr>
<tr><td>28</td><td>RoboDual</td><td>2024.10</td><td>上海交大, 上海AI Lab, 港大, AgiBot</td><td>是</td><td><a href="https://arxiv.org/abs/2410.08001">arXiv:2410.08001</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation<br><strong>规模</strong> 专家20M + VLA &nbsp; <strong>输入</strong> RGB图像, 语言提示 &nbsp; <strong>输出</strong> 机器人动作（多步）<br><strong>核心贡献</strong> 通才VLA+专家扩散策略双系统，真实任务提升26.7%，频率提升3.8倍</div></details></td></tr>
<tr><td>29</td><td>CogACT</td><td>2024.11</td><td>Microsoft Research Asia, 清华, USTC</td><td>是</td><td><a href="https://arxiv.org/abs/2411.19650">arXiv:2411.19650</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation<br><strong>规模</strong> ~7B &nbsp; <strong>输入</strong> RGB图像, 语言指令 &nbsp; <strong>输出</strong> 连续动作序列（扩散Transformer）<br><strong>核心贡献</strong> 组件化VLA架构，解耦认知与动作预测，仿真成功率+35%，实机+55%</div></details></td></tr>
<tr><td>30</td><td>GR-2</td><td>2024.10</td><td>ByteDance Research</td><td>部分</td><td><a href="https://arxiv.org/abs/2410.06158">arXiv:2410.06158</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 视频帧, 语言指令 &nbsp; <strong>输出</strong> 机器人动作, 视频生成<br><strong>核心贡献</strong> 38M视频clips预训练捕获世界动态，100+任务97.7%平均成功率</div></details></td></tr>
<tr><td>31</td><td>VPP</td><td>2024.12</td><td>清华, UC Berkeley, 上海AI Lab, Robot Era</td><td>是</td><td><a href="https://arxiv.org/abs/2412.14803">arXiv:2412.14803</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations<br><strong>规模</strong> — &nbsp; <strong>输入</strong> RGB图像, 语言指令 &nbsp; <strong>输出</strong> 机器人动作<br><strong>核心贡献</strong> 利用视频扩散模型的预测视觉表征学习控制，CALVIN提升18.6%</div></details></td></tr>
<tr><td>32</td><td>RoboVLMs</td><td>2024.12</td><td>清华, ByteDance, CASIA, 上海交大, NUS</td><td>是</td><td><a href="https://arxiv.org/abs/2412.14058">arXiv:2412.14058</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models<br><strong>规模</strong> — &nbsp; <strong>输入</strong> RGB图像, 语言指令 &nbsp; <strong>输出</strong> 机器人动作<br><strong>核心贡献</strong> 系统性研究VLA设计选择，600+实验、8+VLM骨干、4种策略架构</div></details></td></tr>
<tr><td>33</td><td>TraceVLA</td><td>2024.12</td><td>UMD, Microsoft Research</td><td>是</td><td><a href="https://arxiv.org/abs/2412.10345">arXiv:2412.10345</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies<br><strong>规模</strong> 7B / 4B &nbsp; <strong>输入</strong> RGB图像（叠加视觉轨迹）, 语言指令 &nbsp; <strong>输出</strong> 机器人动作<br><strong>核心贡献</strong> 视觉轨迹提示增强时空感知，实机任务性能提升3.5倍</div></details></td></tr>
<tr><td>34</td><td>FLIP</td><td>2024.12</td><td>NUS</td><td>—</td><td><a href="https://arxiv.org/abs/2412.08261">arXiv:2412.08261</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> FLIP: Flow-Centric Generative Planning as General-Purpose Manipulation World Model<br><strong>规模</strong> — &nbsp; <strong>输入</strong> RGB图像, 语言指令 &nbsp; <strong>输出</strong> 图像流, 视频计划, 机器人动作<br><strong>核心贡献</strong> 基于模型规划框架，图像流作为通用动作表示，合成长时域计划</div></details></td></tr>
<tr><td>35</td><td>DiVLA</td><td>2024.12</td><td>Midea Group等</td><td>—</td><td><a href="https://arxiv.org/abs/2412.03293">arXiv:2412.03293</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Diffusion-VLA: Generalizable and Interpretable Robot Foundation Model via Self-Generated Reasoning<br><strong>规模</strong> 2B–72B &nbsp; <strong>输入</strong> RGB图像, 语言指令 &nbsp; <strong>输出</strong> 机器人动作（扩散模型）+ 自生成推理<br><strong>核心贡献</strong> 统一自回归模型（推理）与扩散模型（动作），推理注入模块</div></details></td></tr>
</tbody>
</table>

---

## 三、快速发展阶段（2025年）

<table style="border-collapse: collapse; width: 100%;">
<thead><tr><th>序号</th><th>工作名称</th><th>提出时间</th><th>研究机构</th><th>是否开源</th><th>链接</th><th>详情</th></tr></thead>
<tbody>
<tr><td>36</td><td>SpatialVLA</td><td>2025.01</td><td>上海AI Lab, ShanghaiTech, TeleAI</td><td>是</td><td><a href="https://arxiv.org/abs/2501.15830">arXiv:2501.15830</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Models<br><strong>规模</strong> 3.5B &nbsp; <strong>输入</strong> 图像观测, 语言指令 &nbsp; <strong>输出</strong> 机器人动作(7D)<br><strong>核心贡献</strong> 引入Ego3D位置编码注入3D空间信息，自适应动作网格离散化</div></details></td></tr>
<tr><td>37</td><td>UP-VLA</td><td>2025.01 (ICML 2025)</td><td>清华大学, 上海齐智研究所</td><td>是</td><td><a href="https://arxiv.org/abs/2501.18867">arXiv:2501.18867</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 图像观测, 语言指令 &nbsp; <strong>输出</strong> 机器人动作, 未来图像预测<br><strong>核心贡献</strong> 统一多模态理解与未来预测的训练范式</div></details></td></tr>
<tr><td>38</td><td>π₀-FAST</td><td>2025.01</td><td>Physical Intelligence, UC Berkeley, Stanford</td><td>是</td><td><a href="https://arxiv.org/abs/2501.09747">arXiv:2501.09747</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> FAST: Efficient Action Tokenization for Vision-Language-Action Models<br><strong>规模</strong> 基于π₀ (3B骨干) &nbsp; <strong>输入</strong> 图像观测, 语言指令 &nbsp; <strong>输出</strong> 机器人动作（FAST tokenization）<br><strong>核心贡献</strong> 频域动作序列token化(DCT)，训练速度提升5倍，保持精度</div></details></td></tr>
<tr><td>39</td><td>OpenVLA-OFT</td><td>2025.02</td><td>Stanford University</td><td>是</td><td><a href="https://arxiv.org/abs/2502.19645">arXiv:2502.19645</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success<br><strong>规模</strong> 基于OpenVLA (7B) &nbsp; <strong>输入</strong> 图像（第三方/腕部相机）, 语言指令, 本体感知 &nbsp; <strong>输出</strong> 连续机器人动作<br><strong>核心贡献</strong> 优化微调方案(OFT)：并行解码+动作块+连续表示，推理加速25-50倍</div></details></td></tr>
<tr><td>40</td><td>UniAct</td><td>2025 (CVPR 2025)</td><td>—</td><td>是</td><td><a href="https://arxiv.org/abs/2512.24321">arXiv:2512.24321</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Universal Actions for Enhanced Embodied Foundation Models<br><strong>规模</strong> 0.5B &nbsp; <strong>输入</strong> 图像观测, 语言指令 &nbsp; <strong>输出</strong> 通用动作→机器人特定命令<br><strong>核心贡献</strong> 统一异构动作表示的通用动作空间，学习跨机器人通用原子行为</div></details></td></tr>
<tr><td>41</td><td>ARM4R</td><td>2025.02 (ICML 2025)</td><td>UC Berkeley</td><td>是</td><td><a href="https://arxiv.org/abs/2502.13142">arXiv:2502.13142</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Pre-training Auto-regressive Robotic Models with 4D Representations<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 自我中心人类视频, 机器人演示 &nbsp; <strong>输出</strong> 机器人本体状态和动作<br><strong>核心贡献</strong> 4D表征（3D点跟踪随时间）从人类视频预训练机器人模型</div></details></td></tr>
<tr><td>42</td><td>π₀.₅</td><td>2025.04 (CoRL 2025)</td><td>Physical Intelligence</td><td>—</td><td><a href="https://arxiv.org/abs/2504.16054">arXiv:2504.16054</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> π₀.₅: a Vision-Language-Action Model with Open-World Generalization<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 图像, 语言, 物体检测, 语义子任务 &nbsp; <strong>输出</strong> 低级机器人动作, 中间子任务预测<br><strong>核心贡献</strong> 异构任务协同训练实现开放世界泛化，可执行10-15分钟的长时域任务</div></details></td></tr>
<tr><td>43</td><td>NORA</td><td>2025.04</td><td>—</td><td>是</td><td><a href="https://arxiv.org/abs/2504.19854">arXiv:2504.19854</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks<br><strong>规模</strong> 3B &nbsp; <strong>输入</strong> 图像观测, 语言指令 &nbsp; <strong>输出</strong> 机器人动作<br><strong>核心贡献</strong> 紧凑3B VLA，基于Qwen-2.5-VL-3B+FAST+tokenizer，性能媲美大模型</div></details></td></tr>
<tr><td>44</td><td>DexVLA</td><td>2025.02</td><td>Midea Group, 华东师范大学</td><td>是</td><td><a href="https://arxiv.org/abs/2502.05855">arXiv:2502.05855</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control<br><strong>规模</strong> 扩散专家1B &nbsp; <strong>输入</strong> 视觉, 语言, 机器人状态 &nbsp; <strong>输出</strong> 机器人动作（扩散动作专家）<br><strong>核心贡献</strong> 插件式扩散动作专家(1B)+具身课程学习，支持单臂/双臂/灵巧手</div></details></td></tr>
<tr><td>45</td><td>ChatVLA</td><td>2025.02 (EMNLP 2025)</td><td>Midea Group, 华东师范大学, 上海大学, 清华等</td><td>—</td><td><a href="https://arxiv.org/abs/2502.14420">arXiv:2502.14420</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 视觉, 语言, 多模态数据 &nbsp; <strong>输出</strong> 机器人动作 + 多模态理解输出<br><strong>核心贡献</strong> 分阶段对齐训练+MoE解决VLA训练中的遗忘和任务干扰</div></details></td></tr>
<tr><td>46</td><td>Magma</td><td>2025.02 (CVPR 2025)</td><td>Microsoft Research, UMD, UW-Madison, KAIST, UW</td><td>是</td><td><a href="https://arxiv.org/abs/2502.13130">arXiv:2502.13130</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Magma: A Foundation Model for Multimodal AI Agents<br><strong>规模</strong> 8B &nbsp; <strong>输入</strong> 视觉, 语言, 时空信息 &nbsp; <strong>输出</strong> 数字/物理环境中的动作<br><strong>核心贡献</strong> VLM扩展空间时序智能(SoM+ToM)，统一数字和物理世界Agent</div></details></td></tr>
<tr><td>47</td><td>DexGraspVLA</td><td>2025.02 (AAAI 2026 Oral)</td><td>北京大学, HKUST(GZ), UPenn</td><td>是</td><td><a href="https://arxiv.org/abs/2502.20900">arXiv:2502.20900</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 视觉, 语言, 深度信息 &nbsp; <strong>输出</strong> 灵巧抓取动作<br><strong>核心贡献</strong> 层级VLM规划器+扩散控制器，零样本灵巧抓取成功率90%+</div></details></td></tr>
<tr><td>48</td><td>Hi Robot</td><td>2025.02</td><td>Physical Intelligence, Stanford, UC Berkeley</td><td>—</td><td><a href="https://arxiv.org/abs/2502.19417">arXiv:2502.19417</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 视觉, 语言, 实时反馈 &nbsp; <strong>输出</strong> 机器人动作（层级双系统）<br><strong>核心贡献</strong> 层级系统：高层VLM推理分解+低层π₀执行，支持实时反馈纠正，复杂任务85%成功率</div></details></td></tr>
<tr><td>49</td><td>Helix</td><td>2025.02/05</td><td>Figure AI</td><td>部分</td><td><a href="https://arxiv.org/abs/2505.03912">arXiv:2505.03912</a> (OpenHelix)</td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Helix: A Vision-Language-Action Model for Generalist Humanoid Control<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 视觉, 语言, 本体感知 &nbsp; <strong>输出</strong> 全身人形机器人高频连续控制<br><strong>核心贡献</strong> 首个输出全人形上身高频连续控制的VLA，支持双机器人协作</div></details></td></tr>
<tr><td>50</td><td>Gemini Robotics</td><td>2025.03</td><td>Google DeepMind</td><td>否</td><td><a href="https://arxiv.org/abs/2503.20020">arXiv:2503.20020</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Gemini Robotics: Bringing AI into the Physical World<br><strong>规模</strong> — (基于Gemini 2.0) &nbsp; <strong>输入</strong> 视觉, 语言指令 &nbsp; <strong>输出</strong> 电机控制命令<br><strong>核心贡献</strong> 基于Gemini 2.0的VLA，在通用性、交互性和灵巧性方面取得重大进展</div></details></td></tr>
<tr><td>51</td><td>GR00T N1</td><td>2025.03</td><td>NVIDIA</td><td>是</td><td><a href="https://arxiv.org/abs/2503.14734">arXiv:2503.14734</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> GR00T N1: An Open Foundation Model for Generalist Humanoid Robots<br><strong>规模</strong> 2B / 3B &nbsp; <strong>输入</strong> 视觉, 语言, 本体感知 &nbsp; <strong>输出</strong> 机器人动作（扩散Transformer）<br><strong>核心贡献</strong> 开源人形机器人VLA，双系统架构(VLM+扩散Transformer)，混合真实/合成数据训练</div></details></td></tr>
<tr><td>52</td><td>CoT-VLA</td><td>2025.03 (CVPR 2025)</td><td>Stanford, MIT, NVIDIA等</td><td>是</td><td><a href="https://arxiv.org/abs/2503.22020">arXiv:2503.22020</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models<br><strong>规模</strong> 7B &nbsp; <strong>输入</strong> 视觉, 语言, 机器人观测 &nbsp; <strong>输出</strong> 未来图像帧(视觉目标) + 动作序列<br><strong>核心贡献</strong> 预测未来图像帧作为视觉思维链，实机任务提升17%</div></details></td></tr>
<tr><td>53</td><td>HybridVLA</td><td>2025.03</td><td>北京大学, BAAI, CUHK</td><td>是</td><td><a href="https://arxiv.org/abs/2503.10631">arXiv:2503.10631</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model<br><strong>规模</strong> 7B &nbsp; <strong>输入</strong> 视觉, 语言, 机器人状态 &nbsp; <strong>输出</strong> 机器人动作（协同扩散+自回归）<br><strong>核心贡献</strong> 在单一LLM中统一自回归和扩散动作预测，仿真+14%，实机+19%</div></details></td></tr>
<tr><td>54</td><td>GO-1</td><td>2025.03</td><td>AgiBot, OpenDriveLab, 上海创新研究院</td><td>是</td><td><a href="https://arxiv.org/abs/2503.06669">arXiv:2503.06669</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> GO-1 (AgiBot World Colosseo)<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 视觉, 语言, 机器人状态 &nbsp; <strong>输出</strong> 潜在动作token (ViLLA框架)<br><strong>核心贡献</strong> 100+台G1机器人1M+轨迹训练，比RDT提升32%，比OXE策略提升30%</div></details></td></tr>
<tr><td>55</td><td>GR00T N1 (上方已列)</td><td>—</td><td>—</td><td>—</td><td>—</td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> —<br><strong>规模</strong> — &nbsp; <strong>输入</strong> — &nbsp; <strong>输出</strong> —<br><strong>核心贡献</strong> —</div></details></td></tr>
<tr><td>56</td><td>ChatVLA-2</td><td>2025.05 (NeurIPS 2025)</td><td>Midea Group, 华东师范大学</td><td>—</td><td><a href="https://arxiv.org/abs/2505.21906">arXiv:2505.21906</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 图像观测, 语言指令 &nbsp; <strong>输出</strong> 机器人动作 + 推理输出<br><strong>核心贡献</strong> MoE架构+两阶段训练保留VLM能力，支持开放世界具身推理</div></details></td></tr>
<tr><td>57</td><td>OneTwoVLA</td><td>2025.05</td><td>—</td><td>是</td><td><a href="https://arxiv.org/abs/2505.11917">arXiv:2505.11917</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 图像观测, 语言指令 &nbsp; <strong>输出</strong> 机器人动作 + 推理token<br><strong>核心贡献</strong> 自适应切换System 1/2，在关键时刻触发显式推理</div></details></td></tr>
<tr><td>58</td><td>SmolVLA</td><td>2025.06</td><td>Hugging Face (LeRobot)</td><td>是</td><td><a href="https://arxiv.org/abs/2506.01844">arXiv:2506.01844</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics<br><strong>规模</strong> 450M &nbsp; <strong>输入</strong> 多相机视角, 机器人感觉运动状态, 语言指令 &nbsp; <strong>输出</strong> 连续机器人动作<br><strong>核心贡献</strong> 紧凑450M VLA，性能媲美10倍大模型，单GPU训练和消费级硬件部署</div></details></td></tr>
<tr><td>59</td><td>UniVLA</td><td>2025.05</td><td>OpenDriveLab (BAAI Vision)</td><td>是</td><td><a href="https://arxiv.org/abs/2505.06111">arXiv:2505.06111</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> UniVLA: Learning to Act Anywhere with Task-centric Latent Actions<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 视觉, 语言, 异构数据 &nbsp; <strong>输出</strong> 潜在动作token<br><strong>核心贡献</strong> 任务中心潜在动作学习，仅需960 A100-hours (vs 21,500)，跨具身泛化</div></details></td></tr>
<tr><td>60</td><td>GraspVLA</td><td>2025.05 (CoRL 2025)</td><td>北京大学, 港大, BAAI, Galbot</td><td>部分</td><td><a href="https://arxiv.org/abs/2505.03233">arXiv:2505.03233</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 视觉, 语言, 合成抓取数据 &nbsp; <strong>输出</strong> 机器人抓取动作<br><strong>核心贡献</strong> 首个在十亿级合成数据(SynGrasp-1B)上预训练的抓取VLA</div></details></td></tr>
<tr><td>61</td><td>Robot-R1</td><td>2025.05</td><td>KAIST, Yonsei, UC Berkeley</td><td>—</td><td><a href="https://arxiv.org/abs/2506.00070">arXiv:2506.00070</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics<br><strong>规模</strong> 7B &nbsp; <strong>输入</strong> 场景图像, 环境元数据, 语言指令 &nbsp; <strong>输出</strong> 关键点状态预测, 机器人动作<br><strong>核心贡献</strong> RL增强具身推理，7B模型超越GPT-4o的空间/运动推理能力</div></details></td></tr>
<tr><td>62</td><td>3D-CAVLA</td><td>2025.05 (CVPR 2025 Workshop)</td><td>NYU</td><td>是</td><td><a href="https://arxiv.org/abs/2505.05800">arXiv:2505.05800</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> 3D-CAVLA: Leveraging Depth and 3D Context to Generalize VLA Models for Unseen Tasks<br><strong>规模</strong> 基于LLaMA2-7B &nbsp; <strong>输入</strong> 视觉, 语言, 深度, 3D上下文 &nbsp; <strong>输出</strong> 机器人操作动作<br><strong>核心贡献</strong> 链式思维推理+深度感知+任务导向ROI检测，LIBERO 98.1%，未见任务+8.8%</div></details></td></tr>
<tr><td>63</td><td>WorldVLA</td><td>2025.06</td><td>阿里巴巴达摩院</td><td>是</td><td><a href="https://arxiv.org/abs/2506.21539">arXiv:2506.21539</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> WorldVLA: Towards Autoregressive Action World Model<br><strong>规模</strong> 7B &nbsp; <strong>输入</strong> 视觉, 语言, 机器人状态 &nbsp; <strong>输出</strong> 机器人动作 + 未来图像预测<br><strong>核心贡献</strong> 统一VLA与世界模型，注意力掩码策略缓解自回归误差传播</div></details></td></tr>
<tr><td>64</td><td>Hume</td><td>2025.05</td><td>上海交大, 上海AI Lab, 浙大, AgiBot, 复旦</td><td>是</td><td><a href="https://arxiv.org/abs/2505.21432">arXiv:2505.21432</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Hume: Introducing System-2 Thinking in Visual-Language-Action Model<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 视觉, 语言, 机器人状态 &nbsp; <strong>输出</strong> 机器人动作（双系统架构）<br><strong>核心贡献</strong> 引入System-2思考：价值引导低频深度思考+System-1实时级联动作去噪</div></details></td></tr>
<tr><td>65</td><td>SP-VLA</td><td>2025.06</td><td>—</td><td>—</td><td><a href="https://arxiv.org/abs/2506.12723">arXiv:2506.12723</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 图像, 语言指令 &nbsp; <strong>输出</strong> 机器人动作<br><strong>核心贡献</strong> 动作感知模型调度+时空语义双感知token剪枝，加速1.5-2.4倍</div></details></td></tr>
<tr><td>66</td><td>CoA</td><td>2025.06 (NeurIPS 2025)</td><td>—</td><td>是</td><td><a href="https://arxiv.org/abs/2506.09990">arXiv:2506.09990</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 图像, 语言指令 &nbsp; <strong>输出</strong> 机器人动作轨迹<br><strong>核心贡献</strong> 逆向推理生成动作轨迹（从目标到当前状态），动作级思维链</div></details></td></tr>
<tr><td>67</td><td>BitVLA</td><td>2025.06</td><td>—</td><td>是</td><td><a href="https://arxiv.org/abs/2506.07530">arXiv:2506.07530</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 图像, 语言指令 &nbsp; <strong>输出</strong> 机器人动作<br><strong>核心贡献</strong> 首次将1-bit量化应用于VLA，三值参数{-1,0,1}，仅29.8%内存</div></details></td></tr>
<tr><td>68</td><td>Fast ECoT</td><td>2025.06</td><td>UCL, U. of Freiburg, Cisco Research</td><td>—</td><td><a href="https://arxiv.org/abs/2506.07639">arXiv:2506.07639</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 视觉, 语言, 机器人状态 &nbsp; <strong>输出</strong> 机器人动作（加速ECoT推理）<br><strong>核心贡献</strong> 缓存复用高层推理+并行模块推理+异步调度，延迟降低7.5倍，无需重训</div></details></td></tr>
<tr><td>69</td><td>WorldVLA (上方已列)</td><td>—</td><td>—</td><td>—</td><td>—</td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> —<br><strong>规模</strong> — &nbsp; <strong>输入</strong> — &nbsp; <strong>输出</strong> —<br><strong>核心贡献</strong> —</div></details></td></tr>
<tr><td>70</td><td>TriVLA</td><td>2025.07</td><td>—</td><td>—</td><td><a href="https://arxiv.org/abs/2507.01424">arXiv:2507.01424</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> TriVLA: A Triple-System-Based Unified VLA with Episodic World Modeling for General Robot Control<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 图像, 语言指令 &nbsp; <strong>输出</strong> 机器人动作<br><strong>核心贡献</strong> 三系统架构(策略学习+视觉语言+动态感知)+情景世界模型</div></details></td></tr>
<tr><td>71</td><td>DreamVLA</td><td>2025.07 (NeurIPS 2025)</td><td>—</td><td>是</td><td><a href="https://arxiv.org/abs/2507.04447">arXiv:2507.04447</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> DreamVLA: A VLA Model Dreamed with Comprehensive World Knowledge<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 图像, 语言指令 &nbsp; <strong>输出</strong> 机器人动作 + 世界知识预测（动态区域、深度、语义）<br><strong>核心贡献</strong> 感知-预测-动作环路，预测中间世界知识表征而非图像</div></details></td></tr>
<tr><td>72</td><td>GR-3</td><td>2025.07</td><td>ByteDance Seed (硬件: Fourier Intelligence)</td><td>—</td><td><a href="https://arxiv.org/abs/2507.15493">arXiv:2507.15493</a></td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> GR-3 Technical Report<br><strong>规模</strong> — &nbsp; <strong>输入</strong> 语言指令, 环境观测, 机器人状态 &nbsp; <strong>输出</strong> 双臂移动机器人动作块<br><strong>核心贡献</strong> VLM+动作预测的大规模VLA，多面训练(网络数据+人类VR数据+机器人数据)</div></details></td></tr>
<tr><td>73</td><td>FiS-VLA</td><td>—</td><td>—</td><td>—</td><td>—</td><td><details><summary>详情</summary><div style="font-size:0.9em;margin:0.5em 0;"><strong>论文</strong> —<br><strong>规模</strong> — &nbsp; <strong>输入</strong> — &nbsp; <strong>输出</strong> —<br><strong>核心贡献</strong> 未找到该名称的公开论文</div></details></td></tr>
</tbody>
</table>

---

## 四、统计摘要

| 维度 | 统计 |
|:---|:---|
| 总计模型数 | 约 71 个（FiS-VLA未找到公开论文） |
| 开源比例 | ~60%+ 完全开源 |
| 模型参数范围 | 27M (Octo-Small) → 72B (DiVLA) |
| 最常见参数量 | 7B（OpenVLA, LAPA, TraceVLA, CogACT, CoT-VLA, HybridVLA, WorldVLA, Robot-R1等） |
| 主要输入模态 | 视觉(RGB/深度/点云) + 自然语言指令 + 本体感知 |
| 主要输出模态 | 连续/离散机器人动作，部分同时输出视频/图像预测、推理链 |
| 重要发展趋势 | 扩散模型动作生成、层级/双系统架构、思维链推理、世界模型集成、轻量化/量化、跨具身泛化 |

---

## 五、关键趋势总结

1. **动作生成范式**：从早期的直接回归 → 扩散策略(Diffusion Policy) → Flow Matching(π₀) → 自回归token化(FAST) → 混合方案(HybridVLA)
2. **架构演进**：单一模型 → 双系统(System 1/2) → 三系统(TriVLA) → MoE(ChatVLA)
3. **推理能力**：无推理 → ECoT显式推理 → CoT-VLA视觉思维链 → DreamVLA世界知识预测
4. **数据效率**：从需要大量机器人数据 → 利用互联网视频(LAPA/GR-2) → 合成数据(GraspVLA) → 人类视频(ARM4R)
5. **效率优化**：模型压缩(BitVLA 1-bit)、小型化(SmolVLA 450M)、推理加速(Fast ECoT 7.5×)、token剪枝(SP-VLA)
6. **产业化**：Google(Gemini Robotics)、NVIDIA(GR00T N1)、Physical Intelligence(π₀系列)、ByteDance(GR系列)、Midea(ChatVLA/DexVLA/DiVLA)、Figure AI(Helix)、AgiBot(GO-1)等企业深度参与

---

